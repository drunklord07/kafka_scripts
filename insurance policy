import os
import re
import json
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE = "input.txt"
CHUNK_SIZE = 2000

KEYWORDS = [
    "insurance number", "policy number", "insurance id"
]

OUTPUT_DOCX = "final_output_insuranceid.docx"
OUTPUT_XLSX = "final_output_insuranceid.xlsx"
TEMP_DIR    = "temp_parts"

# Precompile a regex for fallback extraction:
# Matches any keyword from KEYWORDS followed by ':' or '=' then captures the value up to a comma or closing brace
KEYWORD_PATTERN = re.compile(
    r"(?P<key>" + "|".join(re.escape(k) for k in KEYWORDS) + r")\s*[:=]\s*(?P<val>[^,}\n\r]+)",
    flags=re.IGNORECASE
)

# === CLEANER FOR XML-COMPATIBLE STRINGS ===
def clean_xml_string(s: str) -> str:
    return "".join(
        ch
        for ch in s
        if ch in "\n\r\t"
        or (0x20 <= ord(ch) <= 0xD7FF)
        or (0xE000 <= ord(ch) <= 0xFFFD)
    )

# === FIND KEYS IN A PYTHON OBJECT (CASE-INSENSITIVE SUBSTRING MATCH) ===
def find_keys(obj, keywords, path=None):
    """
    Recursively walk a Python dict/list. Whenever a dict key's lowercase
    contains any keyword, capture (path+[key], value).
    """
    if path is None:
        path = []
    matches = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            lowkey = k.strip().lower()
            if any(kw in lowkey for kw in keywords):
                matches.append((path + [k], v))
            matches.extend(find_keys(v, keywords, path + [k]))
    elif isinstance(obj, list):
        for idx, item in enumerate(obj):
            matches.extend(find_keys(item, keywords, path + [str(idx)]))
    return matches

# === INLINE HIGHLIGHT HELPER ===
def add_highlighted_payload(para, payload, found_intervals):
    """
    Given 'found_intervals' = list of (start, end) byte-indexes to highlight in red,
    write the payload into 'para' by splitting it into runs.
    """
    # Merge overlapping intervals
    merged = []
    for s, e in sorted(found_intervals):
        if not merged:
            merged.append([s, e])
        else:
            last = merged[-1]
            if s <= last[1]:
                last[1] = max(last[1], e)
            else:
                merged.append([s, e])

    pos = 0
    for start, end in merged:
        if start > pos:
            para.add_run(clean_xml_string(payload[pos:start]))
        run = para.add_run(clean_xml_string(payload[start:end]))
        run.font.color.rgb = RGBColor(255, 0, 0)
        pos = end
    if pos < len(payload):
        para.add_run(clean_xml_string(payload[pos:]))

# === PARSE RECORDS BY CreateTime LINES ===
def load_records():
    records = []
    last_id = None
    ct_re = re.compile(r"^CreateTime:(\d+)\s+(.*)$")
    with open(INPUT_FILE, "r", encoding="utf-8", errors="ignore") as f:
        for raw in f:
            line = raw.rstrip("\r\n")
            m = ct_re.match(line)
            if m and last_id is not None:
                records.append((last_id, m.group(1), m.group(2)))
            elif line.strip():
                last_id = line.strip()
    return records

def chunk_records(records):
    for i in range(0, len(records), CHUNK_SIZE):
        yield records[i : i + CHUNK_SIZE], i // CHUNK_SIZE

# === WORKER FUNCTION ===
def process_chunk(args):
    chunk, idx, result_list = args
    lc_keys = {kw.lower() for kw in KEYWORDS}

    doc = Document()
    matches_data = []
    seen = 0
    with_ins = 0

    for identifier, ts, payload in chunk:
        seen += 1
        json_matches = []
        fallback_matches = []

        # 1) Try JSON parsing and recursive search
        try:
            obj = json.loads(payload)
        except json.JSONDecodeError:
            obj = None

        if obj is not None:
            found = find_keys(obj, lc_keys)
            for path, val in found:
                field_name = ".".join(path)
                json_matches.append((field_name, str(val)))

        # 2) Regex fallback on raw payload for key= or key:
        for m in KEYWORD_PATTERN.finditer(payload):
            key_text = m.group("key").strip()
            val_text = m.group("val").strip()
            fallback_matches.append((key_text, val_text))

        # Combine, avoiding duplicates
        combined = []
        seen_pairs = set()
        for field_name, val_text in json_matches + fallback_matches:
            pair = (field_name, val_text)
            if pair not in seen_pairs:
                seen_pairs.add(pair)
                combined.append(pair)

        if not combined:
            continue

        with_ins += 1

        # Build the Word paragraph
        para = doc.add_paragraph()
        header = f"{identifier} | CreateTime:{ts} | "
        para.add_run(clean_xml_string(header))

        # Determine intervals in payload to highlight (from both JSON and fallback)
        highlight_intervals = []
        lower_payload = payload.lower()
        for field_name, val_text in combined:
            start_v = payload.find(val_text)
            if start_v != -1:
                highlight_intervals.append((start_v, start_v + len(val_text)))
            key_low = field_name.split(".")[-1].lower()
            pos_k = lower_payload.find(key_low)
            if pos_k != -1:
                highlight_intervals.append((pos_k, pos_k + len(key_low)))

        # Inline highlight in the payload
        add_highlighted_payload(para, payload, highlight_intervals)

        # Summary at the end
        para.add_run(" | value: ")
        for i, (_, val_text) in enumerate(combined):
            if i:
                para.add_run(", ")
            run_val = para.add_run(clean_xml_string(val_text))
            run_val.font.color.rgb = RGBColor(255, 0, 0)

        para.add_run(" | field: ")
        for i, (field_name, _) in enumerate(combined):
            if i:
                para.add_run(", ")
            fr = para.add_run(field_name)
            fr.font.color.rgb = RGBColor(255, 0, 0)

        # Record for Excel
        for field_name, val_text in combined:
            matches_data.append((identifier, ts, payload, val_text, field_name))

    # Save Word chunk if any matches
    if matches_data:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((matches_data, seen, with_ins))

# === MERGE AND WRITE FUNCTIONS ===
def merge_word():
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        if not fn.endswith(".docx"):
            continue
        part = Document(os.path.join(TEMP_DIR, fn))
        for p in part.paragraphs:
            np = merged.add_paragraph()
            for r in p.runs:
                nr = np.add_run(r.text)
                if r.font.color and r.font.color.rgb:
                    nr.font.color.rgb = r.font.color.rgb
                nr.bold, nr.italic, nr.underline = r.bold, r.italic, r.underline
    merged.save(OUTPUT_DOCX)

def write_excel(all_matches):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet()
    red = wb.add_format({"font_color": "red"})

    ws.write_row(0, 0, ["Identifier", "Timestamp", "Payload", "InsuranceID", "Field"])
    r = 1
    for idf, ts, payload, val, field_name in all_matches:
        ws.write(r, 0, idf)
        ws.write(r, 1, ts)
        ws.write(r, 2, payload)
        ws.write(r, 3, val, red)
        ws.write(r, 4, field_name)
        r += 1
    wb.close()

# === MAIN ===
if __name__ == "__main__":
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    print("=== INSURANCEID EXTRACTOR (with fallback for non-JSON payloads) ===")
    records = load_records()
    print(f"Found {len(records)} records.")
    chunks = list(chunk_records(records))

    mgr = Manager()
    results = mgr.list()

    with Pool(min(cpu_count(), len(chunks))) as pool:
        list(
            tqdm(
                pool.imap_unordered(
                    process_chunk,
                    [(chunk, idx, results) for chunk, idx in chunks],
                ),
                total=len(chunks),
                desc="Processing",
            )
        )

    all_matches = []
    total = 0
    matched = 0
    for data, seen, with_ins in results:
        all_matches.extend(data)
        total += seen
        matched += with_ins

    print(f"Records scanned: {total}, with InsuranceID matches: {matched}, total rows: {len(all_matches)}")

    if os.path.isdir(TEMP_DIR):
        merge_word()
    write_excel(all_matches)

    print(f"\n→ Word: {OUTPUT_DOCX}\n→ Excel: {OUTPUT_XLSX}")
