import os
import re
import json
import shutil
from multiprocessing import Pool, Manager, cpu_count
from docx import Document
from docx.shared import RGBColor
import xlsxwriter
from tqdm import tqdm

# === CONFIG ===
INPUT_FILE       = "input.txt"
CHUNK_SIZE       = 2000

# Core 10-digit body (starts 6–9), with optional +91, 91, or 0 prefix
MOBILE_BODY      = r'(?:\+91[\-\s]?|91|0)?[6-9]\d{9}'

# Only match if preceded/followed by JSON delimiters, whitespace, or start/end
MOBILE_REGEX     = (
    rf'(?:^|(?<=[\s\[\]\{{\}},":\']))'
    rf'({MOBILE_BODY})'
    rf'(?=[\s\[\]\{{\}},":\']|$)'
)

# BROAD pattern: same ten‐digit core but ensured NOT surrounded by any other digit
BROAD_REGEX      = re.compile(
    rf'(?<!\d)(?:{MOBILE_BODY})(?!\d)'
)

# Generic key:value regex for JSON (quoted or unquoted)
KV_RE             = re.compile(
    rf'"([^"]+)"\s*:\s*"(?:{MOBILE_BODY})"|'       # "key":"<mobile>"
    rf'"([^"]+)"\s*:\s*({MOBILE_BODY})(?=[\s,}}\\]]|$)'  # "key":<mobile>
)

# XML tag regex for dynamic key names (phone/mobile/customer/cust/contact),
# quoted or unquoted value = <mobile>
XML_RE            = re.compile(
    rf'<(?:tagname|name)\s*=\s*"(?:(?:phone|mobile|customer|cust|contact))"\s+value\s*=\s*"?({MOBILE_BODY})"?',
    flags=re.IGNORECASE
)
# Fallback: any mobile="..."
XML_ATTR_RE       = re.compile(
    rf'mobile\s*=\s*"?({MOBILE_BODY})"?',
    flags=re.IGNORECASE
)

# Nested-JSON "name":"mobile","value":... (quoted or unquoted)
NESTED_JSON_RE    = re.compile(
    rf'"name"\s*:\s*"(?:(?:phone|mobile|customer|cust|contact))"\s*,\s*"value"\s*:\s*"?({MOBILE_BODY})"?',
    flags=re.IGNORECASE
)

# Dynamic fallback: any key containing phone/mobile/customer/cust/contact in JSON
KEYWORD_MOBILE_RE = re.compile(
    rf'"([^"]*(?:phone|mobile|customer|cust|contact)[^"]*)"\s*[:=]\s*"?({MOBILE_BODY})"?',
    flags=re.IGNORECASE
)

OUTPUT_DOCX       = "final_output_mobile_dynamic.docx"
OUTPUT_XLSX       = "final_output_mobile_dynamic.xlsx"
ERROR_LINES_TXT   = "error_lines.txt"
TEMP_DIR          = "temp_parts"

# === UTILITIES ===
def clean_text(s: str) -> str:
    # Remove null bytes which can break Word/XML
    return s.replace('\x00', '')

def flatten_json(obj, prefix=""):
    """
    Recursively flatten a dict or list. If a string value itself is JSON,
    parse and flatten that too. Primitives in lists recorded as key[index].
    """
    flat = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            path = f"{prefix}{k}"
            if isinstance(v, dict):
                flat.update(flatten_json(v, path + "."))
            elif isinstance(v, list):
                for i, item in enumerate(v):
                    if isinstance(item, (dict, list)):
                        flat.update(flatten_json(item, f"{path}[{i}]."))
                    else:
                        flat[f"{path}[{i}]"] = item
            elif isinstance(v, str) and v.strip().startswith(("{", "[")):
                try:
                    inner = json.loads(v)
                except:
                    flat[path] = v
                else:
                    flat.update(flatten_json(inner, path + "."))
            else:
                flat[path] = v
    elif isinstance(obj, list):
        for i, item in enumerate(obj):
            if isinstance(item, (dict, list)):
                flat.update(flatten_json(item, f"{prefix}[{i}]."))
            else:
                flat[f"{prefix}[{i}]"] = item
    return flat

# === LOAD & GROUP RECORDS ===
def load_records():
    """
    Reads INPUT_FILE. Lines not starting with CreateTime are identifiers.
    Lines like "CreateTime:<ts> <payload>" pair with the previous identifier.
    """
    records = []
    last_id = None
    ct_re   = re.compile(r"^CreateTime:(\d+)\s+(.*)$")
    with open(INPUT_FILE, 'r', encoding='utf-8', errors='ignore') as f:
        for raw in f:
            line = raw.rstrip('\r\n')
            m = ct_re.match(line)
            if m and last_id is not None:
                ts      = m.group(1)
                payload = clean_text(m.group(2))
                records.append((last_id, ts, payload))
            elif line.strip():
                last_id = line.strip()
    return records

# === SPLIT INTO CHUNKS ===
def chunk_records(records):
    for i in range(0, len(records), CHUNK_SIZE):
        yield records[i:i+CHUNK_SIZE], i // CHUNK_SIZE

# === PROCESS ONE CHUNK ===
def process_chunk(args):
    chunk, idx, result_list, error_list = args
    pat_mobile = re.compile(MOBILE_REGEX)
    doc = Document()
    matches_data = []
    recs_seen = recs_with_mobile = 0

    for identifier, ts, payload in chunk:
        recs_seen += 1
        raw = clean_text(payload)

        # 1) Broad check: any exact-10-digit (with prefix) NOT surrounded by other digits?
        broad_hit = bool(BROAD_REGEX.search(raw))
        hits = list(pat_mobile.finditer(raw))
        if not hits:
            if broad_hit:
                # BOUNDARY‐aware MOBILE_REGEX failed → log to error_lines.txt
                error_list.append(f"{identifier} | CreateTime:{ts} | {raw}")
            continue
        recs_with_mobile += 1

        # 2) Attempt to flatten valid JSON
        flat = {}
        try:
            obj = json.loads(raw)
            flat = flatten_json(obj)
        except json.JSONDecodeError:
            pass

        # 3) XML <name="mobile" value=...> (or tagname) with dynamic key
        xml_map = {m.group(1): "mobile" for m in XML_RE.finditer(raw)}
        # 3b) fallback for any mobile="..."
        xml_attr_map = {m.group(1): "mobile" for m in XML_ATTR_RE.finditer(raw)}

        # 4) Nested JSON "name":"mobile","value":...
        nested_map = {m.group(1): "mobile" for m in NESTED_JSON_RE.finditer(raw)}

        # 5) Prepare unescaped text for KV_RE and KEYWORD_MOBILE_RE
        raw_unesc = raw.replace('\\"', '"')

        # Build Word paragraph (highlight mobiles & fields)
        para = doc.add_paragraph(f"{identifier} | CreateTime:{ts} | ")
        last = 0
        fields = []

        for m in hits:
            s, e = m.span(1)
            if s > last:
                para.add_run(raw[last:s])
            mobile = m.group(1)
            run = para.add_run(mobile)
            run.font.color.rgb = RGBColor(255, 0, 0)
            last = e

            # --- Field lookup cascade ---
            field = ""

            # a) Exact or substring match in flattened JSON
            for path, val in flat.items():
                if str(val) == mobile or (isinstance(val, str) and mobile in val):
                    field = path
                    break

            # b) Generic key:value regex on unescaped JSON
            if not field:
                m1 = re.search(r'"([^"]+)"\s*:\s*"' + re.escape(mobile) + r'"', raw_unesc)
                if m1:
                    field = m1.group(1)
                else:
                    m2 = re.search(r'"([^"]+)"\s*:\s*' + re.escape(mobile) + r'(?=[\s,}\]]|$)', raw_unesc)
                    if m2:
                        field = m2.group(1)

            # c) Dynamic key fallback (phone/mobile/customer/cust/contact)
            if not field:
                km = KEYWORD_MOBILE_RE.search(raw_unesc)
                if km and km.group(2) == mobile:
                    field = km.group(1)

            # d) Nested JSON fallback → field = "mobile"
            if not field and mobile in nested_map:
                field = nested_map[mobile]

            # e) XML tag fallback → field = "mobile"
            if not field and mobile in xml_map:
                field = xml_map[mobile]

            # f) XML attribute fallback → field = "mobile"
            if not field and mobile in xml_attr_map:
                field = xml_attr_map[mobile]

            fields.append(field)
            matches_data.append((identifier, ts, raw, mobile, field))

        if last < len(raw):
            para.add_run(raw[last:])

        # Append field names (in red)
        para.add_run(" | field: ")
        for i, fld in enumerate(fields):
            if i:
                para.add_run(", ")
            fr = para.add_run(fld)
            fr.font.color.rgb = RGBColor(255, 0, 0)

    # Save Word chunk if any matches found
    if matches_data:
        os.makedirs(TEMP_DIR, exist_ok=True)
        doc.save(os.path.join(TEMP_DIR, f"chunk_{idx}.docx"))

    result_list.append((matches_data, recs_seen, recs_with_mobile))

# === MERGE WORD CHUNKS ===
def merge_word():
    merged = Document()
    for fn in tqdm(sorted(os.listdir(TEMP_DIR)), desc="Merging Word"):
        if not fn.endswith(".docx"):
            continue
        sub = Document(os.path.join(TEMP_DIR, fn))
        for para in sub.paragraphs:
            out_p = merged.add_paragraph()
            for run in para.runs:
                nr = out_p.add_run(run.text)
                if run.font.color and run.font.color.rgb:
                    nr.font.color.rgb = run.font.color.rgb
                nr.bold, nr.italic, nr.underline = run.bold, run.italic, run.underline
    merged.save(OUTPUT_DOCX)

# === WRITE EXCEL (Matches + Error Table) ===
def write_excel(all_matches, error_list):
    wb = xlsxwriter.Workbook(OUTPUT_XLSX)
    ws = wb.add_worksheet("Mobiles + Errors")
    red = wb.add_format({'font_color':'red'})
    bold = wb.add_format({'bold': True})

    # 1) Header for valid matches
    ws.write_row(0, 0, ["Identifier","Timestamp","Payload","Mobile","Field"], bold)
    row = 1
    for ident, ts, payload, mobile, field in all_matches:
        ws.write(row, 0, ident)
        ws.write(row, 1, ts)
        ws.write(row, 2, payload)
        ws.write(row, 3, mobile, red)
        ws.write(row, 4, field)
        row += 1

    # 2) Blank row
    row += 1

    # 3) Header for error records
    ws.write_row(row, 0, ["Error: Identifier","Timestamp","Payload"], bold)
    row += 1

    # 4) Write each error line
    for err in error_list:
        parts = err.split(" | ", 2)
        if len(parts) == 3:
            ws.write(row, 0, parts[0])
            ws.write(row, 1, parts[1])
            ws.write(row, 2, parts[2])
        else:
            # Put entire line into payload column if split fails
            ws.write(row, 2, err)
        row += 1

    wb.close()

# === MAIN ===
if __name__ == "__main__":
    # Clean up any prior temp
    if os.path.isdir(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)

    print("=== MOBILE EXTRACTOR (Dynamic Field Lookup + Error Logging) ===")
    records = load_records()
    print(f"Total records: {len(records)}")

    chunks = list(chunk_records(records))
    manager = Manager()
    results = manager.list()
    error_lines = manager.list()

    # Process in parallel
    with Pool(min(cpu_count(), len(chunks))) as pool:
        list(tqdm(pool.imap_unordered(
            process_chunk,
            [(chunk, idx, results, error_lines) for chunk, idx in chunks]
        ), total=len(chunks), desc="Processing chunks"))

    # Aggregate matches
    all_matches = []
    tot = has = 0
    for data, recs, with_mobile in results:
        all_matches.extend(data)
        tot += recs
        has += with_mobile

    print(f"Records scanned: {tot}, with mobile‐records: {has}, total matches: {len(all_matches)}")

    # Write error lines to text file
    with open(ERROR_LINES_TXT, 'w', encoding='utf-8') as ef:
        for line in error_lines:
            ef.write(line + "\n")

    # Merge Word docs if any
    if os.path.isdir(TEMP_DIR):
        merge_word()

    # Write Excel (valid matches + error table)
    write_excel(all_matches, list(error_lines))

    # Clean up
    shutil.rmtree(TEMP_DIR)

    print(f"\n→ Word saved to {OUTPUT_DOCX}")
    print(f"→ Excel saved to {OUTPUT_XLSX}")
    print(f"→ Error lines saved to {ERROR_LINES_TXT}")
